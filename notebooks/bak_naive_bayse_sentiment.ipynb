{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make directory for downloading the file to /Users/smap3/PycharmProjects/naive_bayse_anatomy.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin downloading the Moview Review Data dataset from http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset file is saved to /Users/smap3/PycharmProjects/naive_bayse_anatomy/data/moview_review_data/review_polarity.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting negative data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nExtracting positive data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nShuffle the extracted dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done all process! Make below files at /Users/smap3/PycharmProjects/naive_bayse_anatomy/data/moview_review_data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " review_polarity.txt\n"
     ]
    }
   ],
   "source": [
    "import chazutsu\n",
    "\n",
    "r = chazutsu.datasets.MovieReview.polarity().download(force=True, test_size=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer.fit_transform\n",
    "\n",
    "Learn the vocabulary dictionary and return term-document matrix.\n",
    "\n",
    "単語を抽出し、ボキャブラリ(単語の一覧)として行列で返す。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# BoWに変換する\n",
    "count_vect = CountVectorizer(token_pattern=r'[A-Za-z_]+')\n",
    "X_train_counts = count_vect.fit_transform(r.data().review)\n",
    "\n",
    "# 疎行列(csr-matrix)となっている。内容確認する。\n",
    "f = open('X_train_counts.txt', 'w')\n",
    "\n",
    "# dataは文書における各単語の出現回数\n",
    "for idx in range(len(X_train_counts.data)):\n",
    "    f.writelines(str(X_train_counts.data[idx]) + \" \")\n",
    "f.writelines(str(\"\\n\"))\n",
    "\n",
    "# indicesは出現数が0ではない何列目の要素(単語)かを表す\n",
    "for idx in range(len(X_train_counts.indices)):\n",
    "    f.writelines(str(X_train_counts.indices[idx]) + \" \")\n",
    "f.writelines(str(\"\\n\"))\n",
    "\n",
    "# indptrはどこまで同一の行(文書)かを表す\n",
    "for idx in range(len(X_train_counts.indptr)):\n",
    "    f.writelines(str(X_train_counts.indptr[idx]) + \" \")\n",
    "f.writelines(str(\"\\n\"))\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回の訓練データ全体の単語数は39204種類。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39204"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "\n",
    "一つ目の文書にスポットを当てて確認する。\n",
    "単語の種類と出現数を確認。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dictにおける値からkeyを取得する関数。vocaburaly_から単語idを用いて単語名を取得するのに用いる\n",
    "def voc_find(items, term_id):\n",
    "    voc = \"\"\n",
    "    for key, value in items:\n",
    "        if value == term_id:\n",
    "            voc = key\n",
    "            break\n",
    "    return voc\n",
    "\n",
    "\n",
    "# 1つ目の文書の単語の種類と出現数を書き出し\n",
    "f = open('first_document_vocabulary.txt', 'w')\n",
    "\n",
    "for ptr in range(X_train_counts.indptr[1]):    \n",
    "    voc = voc_find(count_vect.vocabulary_.items(), X_train_counts.indices[ptr])\n",
    "\n",
    "    # 単語id 単語名 出現回数を出力\n",
    "    f.writelines(str(X_train_counts.indices[ptr]) + \" \" + str(voc) + \" \" + str(X_train_counts.data[ptr]) + \"\\n\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tfidfで重み付けが行われた結果の確認。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# tfidfによる重み付け処理\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "# 1つ目の文書の単語の種類とf値を書き出し\n",
    "f = open('first_X_train_tfidf.txt', 'w')\n",
    "for ptr in range(X_train_tfidf.indptr[1]):\n",
    "    voc = voc_find(count_vect.vocabulary_.items(), X_train_tfidf.indices[ptr])\n",
    "    \n",
    "    # 単語id 単語名 f値を出力\n",
    "    f.writelines(str(X_train_tfidf.indices[ptr]) + \" \" + str(voc) + \" \" + str(X_train_tfidf.data[ptr]) + \"\\n\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ナイーブベイズ分類を実行しつつ、各パラメータを確認\n",
    "\n",
    "feature_count_  \n",
    "class_count_  \n",
    "class_log_prior_  \n",
    "feature_log_prob_  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, r.data().polarity)\n",
    "\n",
    "# 単語毎に各クラスにおけるの尤度を取得\n",
    "f = open('feature_log_prob_.txt', 'w')\n",
    "for i in range(len(clf.feature_log_prob_[0])):\n",
    "    voc = voc_find(count_vect.vocabulary_.items(), i)\n",
    "\n",
    "    # 単語名 ネガティブクラスにおける各単語の条件付き確率 ポジティブクラスにおける各単語の条件付き確率 \n",
    "    f.writelines(voc + \" \" + str(clf.feature_log_prob_[0][i]) + \" \" + str(clf.feature_log_prob_[1][i]) + \"\\n\") \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この後予測する際は以下となる。\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "ここからpredictの処理を見ていく。\n",
    "predictは以下の通り。\n",
    "\n",
    "        jll = self._joint_log_likelihood(X)\n",
    "        return self.classes_[np.argmax(jll, axis=1)]\n",
    "\n",
    "_joint_log_likelihoodで各クラスの確率を算出し、\n",
    "そのうち最も高いクラスを推定クラスとしている。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_joint_log_likelihoodの処理は以下の通り。\n",
    "\n",
    "        check_is_fitted(self, \"classes_\")\n",
    "\n",
    "        X = check_array(X, accept_sparse='csr')\n",
    "        return (safe_sparse_dot(X, self.feature_log_prob_.T) +\n",
    "                self.class_log_prior_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確率を算出している以下処理の確認。\n",
    "\n",
    "safe_sparse_dot(X, self.feature_log_prob_.T) + self.class_log_prior_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-10.12919246 -10.80435366]\n [-10.79022431 -10.87206429]\n [-10.79022431 -10.87206429]\n ..., \n [-10.81865448 -10.81494686]\n [-10.87230098 -10.746948  ]\n [-10.81170821 -10.87206429]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(39204, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#単語×クラスの行列\n",
    "print(clf.feature_log_prob_.T)\n",
    "\n",
    "clf.feature_log_prob_.T.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文書毎に各クラスの推定確率を算出する。\n",
    "\n",
    "文書中出現単語と単語毎のベイズ確率の行列積を求める。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#safe_sparse_dot(X_train_tfidf, clf.feature_log_prob_.T)\n",
    "\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "\n",
    "def ssd(a, b, dense_output=False):\n",
    "    if issparse(a) or issparse(b):\n",
    "        ret = a * b\n",
    "        if dense_output and hasattr(ret, \"toarray\"):\n",
    "            ret = ret.toarray()\n",
    "        return ret\n",
    "    \n",
    "    else:\n",
    "        return fast_dot(a, b)\n",
    "\n",
    "safe_sparse_dot_value = ssd(X_train_tfidf, clf.feature_log_prob_.T)\n",
    "\n",
    "safe_sparse_dot_value.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各クラスの事前確率がclass_log_prior_に格納されています。\n",
    "今回はpositive,negativeともに1000文書ずつ、合計2000文書を訓練データとして与えています。\n",
    "それらの確率をlogで算出するとその通りになっていることが分かります。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_log_prior_: [-0.69314718 -0.69314718]\n-0.6931471805599453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-123.45103795, -122.91073791],\n       [-132.35858883, -131.94731774],\n       [-137.12199247, -136.38472042],\n       ..., \n       [-138.57355562, -138.28937654],\n       [ -83.26333521,  -82.21486293],\n       [-101.51080321, -100.61101538]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"class_log_prior_: \" + str(clf.class_log_prior_))\n",
    "\n",
    "print(math.log(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ベイズの定理はP(Y)P(X|Y)だけど、logを取っているので和で求めることができる。\n",
    "文書毎に各クラスの条件付き確率を算出。\n",
    "そしてそのうち確率が高い方を推定クラスとする。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-123.45103795, -122.91073791],\n       [-132.35858883, -131.94731774],\n       [-137.12199247, -136.38472042],\n       ..., \n       [-138.57355562, -138.28937654],\n       [ -83.26333521,  -82.21486293],\n       [-101.51080321, -100.61101538]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = safe_sparse_dot_value + clf.class_log_prior_\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際に関数を使ったものと比較してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-123.45103795, -122.91073791],\n       [-132.35858883, -131.94731774],\n       [-137.12199247, -136.38472042],\n       ..., \n       [-138.57355562, -138.28937654],\n       [ -83.26333521,  -82.21486293],\n       [-101.51080321, -100.61101538]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.utils import check_array\n",
    "\n",
    "\n",
    "check_is_fitted(clf, \"classes_\")\n",
    "X_train_tfidf = check_array(X_train_tfidf, accept_sparse='csr')\n",
    "\n",
    "clf._joint_log_likelihood(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "完全に一致していることが分かる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.mean(score == clf._joint_log_likelihood(X_train_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}