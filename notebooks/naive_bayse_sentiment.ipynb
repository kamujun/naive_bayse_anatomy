{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ナイーブベイズによる文書判別における尤度確認\n",
    "\n",
    "scikit-learnで文書判別を行なう際に、判別に影響する単語の尤度がどれくらいであるか確認する。\n",
    "また、scikit-learnの処理中における変数内のデータを出力することにより、実装の理解を深める。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import chazutsu\n",
    "\n",
    "r = chazutsu.datasets.MovieReview.polarity().download(force=True, test_size=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer.fit_transform\n",
    "\n",
    "訓練データである文書集合から単語を抽出し、ボキャブラリ(単語一覧)を行列として返す。\n",
    "行列は疎行列(スパース行列)であり、scipyのcsc_matrix型で格納されている。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# BoWに変換する\n",
    "count_vect = CountVectorizer(token_pattern=r'[A-Za-z_]+')\n",
    "X_train_counts = count_vect.fit_transform(r.data().review)\n",
    "\n",
    "# 疎行列(csr-matrix)となっている。内容確認する。\n",
    "f = open('data/X_train_counts.txt', 'w')\n",
    "\n",
    "# dataは文書における各単語の出現回数\n",
    "for idx in range(len(X_train_counts.data)):\n",
    "    f.writelines(str(X_train_counts.data[idx]) + \" \")\n",
    "f.writelines(str(\"\\n\"))\n",
    "\n",
    "# indicesは出現数が0ではない何列目の要素(単語)か\n",
    "for idx in range(len(X_train_counts.indices)):\n",
    "    f.writelines(str(X_train_counts.indices[idx]) + \" \")\n",
    "f.writelines(str(\"\\n\"))\n",
    "\n",
    "# indptrはどこまで同一の行(文書)か\n",
    "for idx in range(len(X_train_counts.indptr)):\n",
    "    f.writelines(str(X_train_counts.indptr[idx]) + \" \")\n",
    "f.writelines(str(\"\\n\"))\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回の訓練データ全体の単語は39204種類。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39204"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習\n",
    "\n",
    "1つ目の文書にスポットを当てて確認する。\n",
    "1つ目の文書に出てくる単語の種類と出現数を確認。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dictにおける値からkeyを取得する関数。vocaburaly_から単語idを用いて単語名を取得するのに用いる\n",
    "def dict_value_to_key(items, in_val):\n",
    "    ret_key = \"\"\n",
    "    for key, value in items:\n",
    "        if value == in_val:\n",
    "            ret_key = key\n",
    "            break\n",
    "    return ret_key\n",
    "\n",
    "\n",
    "# 1つ目の文書の単語の種類と出現数を書き出し\n",
    "f = open('data/first_document_vocabulary.txt', 'w')\n",
    "\n",
    "for ptr in range(X_train_counts.indptr[1]):    \n",
    "    voc = dict_value_to_key(count_vect.vocabulary_.items(), X_train_counts.indices[ptr])\n",
    "\n",
    "    # 出力形式:単語id 単語名 出現回数を出力\n",
    "    f.writelines(str(X_train_counts.indices[ptr]) + \" \" + str(voc) + \" \" + str(X_train_counts.data[ptr]) + \"\\n\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ナイーブベイズ分類を実行。学習した単語の尤度を確認。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_counts, r.data().polarity)\n",
    "\n",
    "# 単語毎に各クラスにおけるの尤度を取得\n",
    "f = open('data/feature_log_prob_.txt', 'w')\n",
    "for i in range(len(clf.feature_log_prob_[0])):\n",
    "    voc = dict_value_to_key(count_vect.vocabulary_.items(), i)\n",
    "\n",
    "    # 出力形式:単語名 ネガティブクラスにおける各単語の条件付き確率 ポジティブクラスにおける各単語の条件付き確率 \n",
    "    f.writelines(voc + \" \" + str(clf.feature_log_prob_[0][i]) + \" \" + str(clf.feature_log_prob_[1][i]) + \"\\n\") \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文書分類\n",
    "\n",
    "学習した分類器 clf を用いて文書分類を行なうには以下のように実行する。\n",
    "\n",
    "`predicted = clf.predict(X_new_tfidf)`\n",
    "\n",
    "ここからpredictの処理を見ていく。predict内の処理は以下の通り。\n",
    "\n",
    "```\n",
    "        jll = self._joint_log_likelihood(X)\n",
    "        return self.classes_[np.argmax(jll, axis=1)]\n",
    "```\n",
    "\n",
    "_joint_log_likelihoodで各カテゴリの事後確率を算出し、そのうち最も高いクラスを推定クラスとする。(MAP推定)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_joint_log_likelihoodの処理は以下の通り。\n",
    "\n",
    "```\n",
    "        check_is_fitted(self, \"classes_\")\n",
    "\n",
    "        X = check_array(X, accept_sparse='csr')\n",
    "        return (safe_sparse_dot(X, self.feature_log_prob_.T) +\n",
    "                self.class_log_prior_)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事後確率を算出している処理を確認する。\n",
    "\n",
    "`safe_sparse_dot(X, self.feature_log_prob_.T) + self.class_log_prior_`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-10.07997067 -12.41784378]\n [-12.719028   -13.51645606]\n [-12.719028   -13.51645606]\n ..., \n [-12.719028   -12.82330888]\n [-13.41217518 -12.41784378]\n [-12.719028   -13.51645606]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(39204, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 尤度(カテゴリ×単語)の転置行列\n",
    "print(clf.feature_log_prob_.T)\n",
    "\n",
    "clf.feature_log_prob_.T.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テストデータ文書の出現単語と学習していた単語毎の尤度を用いて行列積を求める。\n",
    "求めた結果が推定時の尤度となる。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# safe_sparse_dot(X_train_counts, clf.feature_log_prob_.T)\n",
    "\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "\n",
    "def ssd(a, b, dense_output=False):\n",
    "    if issparse(a) or issparse(b):\n",
    "        ret = a * b\n",
    "        if dense_output and hasattr(ret, \"toarray\"):\n",
    "            ret = ret.toarray()\n",
    "        return ret\n",
    "    \n",
    "    else:\n",
    "        return fast_dot(a, b)\n",
    "\n",
    "safe_sparse_dot_value = ssd(X_train_counts, clf.feature_log_prob_.T)\n",
    "\n",
    "safe_sparse_dot_value.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各クラスの事前確率がclass_log_prior_に格納されている。\n",
    "\n",
    "今回はpositive,negativeともに1000文書ずつ、合計2000文書を訓練データとして与えている。\n",
    "それらの確率をlogで算出するとその通りになっていることが分かる。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_log_prior_: [-0.69314718 -0.69314718]\n-0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print(\"class_log_prior_: \" + str(clf.class_log_prior_))\n",
    "\n",
    "print(math.log(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尤度と事前確率を加算(いずれも対数変換しているので)を行なうことで、事後確率が算出される。\n",
    "1番目の文書を見ると、ネガティブは「-3937.36437245」、ポジティブは「-3904.99122672」なのでポジティブと推定される。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -3937.36437245,  -3904.99122672],\n       [ -4514.96929083,  -4412.54616969],\n       [ -3315.38618094,  -3272.76981737],\n       ..., \n       [ -3624.98324263,  -3694.10381655],\n       [ -7778.33253256,  -7881.31132756],\n       [-10481.45229072, -10453.49552954]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = safe_sparse_dot_value + clf.class_log_prior_\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際にpredictを実行してみると合致することが確認できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = clf.predict(X_train_counts)\n",
    "predicted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}